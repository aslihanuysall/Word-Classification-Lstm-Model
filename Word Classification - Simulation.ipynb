{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import *\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.types import StructType, StringType, DoubleType, IntegerType, StructField\n",
    "import numpy as np\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "import pymssql\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from functools import reduce\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import datetime\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Embedding, BatchNormalization, Input, concatenate, add, multiply, Reshape, Dropout, Activation, LSTM,RNN \n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import model_from_json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utils.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-) Identifies the Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### character-level embedding of words. I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_words = pd.read_csv(\"Armut_ML_Case_-_Eng.csv\")\n",
    "english_words[\"Label\"] = \"English\"\n",
    "english_words.rename(columns = {\"Words\":\"Vocab\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "english_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turkish_words = pd.read_csv(\"Armut_ML_Case_-_Turkish.csv\")\n",
    "turkish_words[\"Label\"] = \"Turkish\"\n",
    "turkish_words.rename(columns = {\"Kelimeler\":\"Vocab\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turkish_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_vocab_df = turkish_words.append(english_words).reset_index(drop=True)\n",
    "raw_vocab_df[\"Vocab\"] = raw_vocab_df[\"Vocab\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_vocab_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove whitespace in raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_vocab_df[\"IsThereWhiteSpace\"] = raw_vocab_df[\"Vocab\"].apply(lambda x: True if ' ' in x else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_vocab_df[\"Vocab\"] = raw_vocab_df[\"Vocab\"].apply(lambda x: x.replace(\" \", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_vocab_df = raw_vocab_df[[\"Vocab\",\"Label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Checking Data Balance\"\"\"\n",
    "\n",
    "print(\"Toplam Türkçe Kelime Sayısı {} \\n\".format(len(turkish_words)), \"Number of English Words {}\".format(len(english_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Checking Number Of Unique Words\"\"\"\n",
    "\n",
    "print(\"Toplam Türkçe Kelime Sayısı {}, Toplam Eşsiz Türkçe Kelime Sayısı {} \\n\".format(len(turkish_words), len(set(turkish_words[\"Vocab\"]))),\n",
    "      \n",
    "      \"Number of English Words {}, Number of Unique English Words {}\".format(len(english_words), len(set(english_words[\"Vocab\"]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_of_turkish_and_english_words = max([max([len(str(kelime)) for kelime in turkish_words[\"Vocab\"]])] +\n",
    "                                              [max([len(str(word)) for word in english_words[\"Vocab\"]])])\n",
    "\n",
    "print(\"max_length_of_turkish_and_english_words = {} \".format(max_length_of_turkish_and_english_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turkish_char_lookup_list = \"a,b,c,ç,d,e,f,g,ğ,h,ı,i,j,k,l,m,n,o,ö,p,r,s,ş,t,u,ü,v,y,z\".split(\",\")\n",
    "english_char_lookup_list = \"a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w,x,y,z, \".split(\",\")\n",
    "char_list = list(set.union(set(turkish_char_lookup_list)).union(set(english_char_lookup_list)))\n",
    "\n",
    "unknown_char = 'UNKNOWN'\n",
    "char_list.insert(0, unknown_char)\n",
    "\n",
    "num_of_char = len(char_list)\n",
    "char_indexes = dict((y, x) for x, y in enumerate(char_list))\n",
    "index_chars = dict((x, y) for x, y in enumerate(char_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"Turkish\", \"English\"]\n",
    "\n",
    "label_indexes = dict((y, x) for x, y in enumerate(labels))\n",
    "index_labels = dict((x, y) for x, y in enumerate(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if there is any char in both Turkish and English raw data that is not in char_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turkish_and_english_unique_letters_set = set()\n",
    "for word in raw_vocab_df[\"Vocab\"]:\n",
    "    turkish_and_english_unique_letters_set = turkish_and_english_unique_letters_set.union(set(str(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turkish_and_english_unique_letters_set - set(char_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_raw_data(raw_vocab_df, label_indexes, max_length_of_vocab):\n",
    "    \n",
    "    X_input_data = np.zeros((len(raw_vocab_df), max_length_of_vocab), np.int32)\n",
    "    y_input_data = []\n",
    "    \n",
    "    for word_index, word in raw_vocab_df.iterrows():\n",
    "        \n",
    "        for char_index, char in enumerate(word[\"Vocab\"]):\n",
    "            if char not in char_list:\n",
    "                X_input_data[word_index, char_index] = char_indexes['UNKNOWN']\n",
    "            else:\n",
    "                X_input_data[word_index, char_index] = char_indexes[char]\n",
    "         \n",
    "        y_input_data.append(label_indexes[word[\"Label\"]])      \n",
    "    \n",
    "    assert X_input_data.shape == (len(X_input_data), max_length_of_vocab)\n",
    "    \n",
    "    return X_input_data.reshape(-1, max_length_of_vocab, 1), np.array(y_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_input_data, y_input_data = preprocess_raw_data(raw_vocab_df, label_indexes, \n",
    "                                            max_length_of_vocab = max_length_of_turkish_and_english_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking shape of Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_input_data shape = {}\".format(X_input_data.shape))\n",
    "print(\"y_input_data shape = {}\".format(y_input_data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train test split based on class label to make balanced seperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_input_data, y_input_data, stratify = y_input_data, test_size=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, stratify = y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"length of X_train = {}\".format(len(X_train)), \"length of X_val = {}\".format(len(X_val)), \"length of X_test = {}\".format(len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\"HP_NUM_UNITS\":64 ,\n",
    "           \"HP_DROPOUT\":0.1 ,\n",
    "           \"HP_OPTIMIZER\":\"RMSprop\"}\n",
    "\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(X_input_data, num_of_unique_chars, embedding_dimension):\n",
    "    \n",
    "    word_embedding_input = Input(shape=(X_input_data.shape[1], X_input_data.shape[2]), dtype='int32')\n",
    "    embedded_input = Embedding(num_of_char, embedding_dimension, input_shape=(X_input_data[1],X_input_data[2]))(word_embedding_input)\n",
    "    reshaped_embedding = Reshape((X_input_data.shape[1],embedding_dimension), name = \"reshape_embedding\")(embedded_input)\n",
    "    \n",
    "    lstm_layer = LSTM(64, return_sequences=False, return_state=False , dropout=0.1)(reshaped_embedding)\n",
    "    \n",
    "    dense_layer = Dense(hparams[\"HP_NUM_UNITS\"], input_dim=lstm_layer.shape[1], kernel_initializer='normal')(lstm_layer)\n",
    "    dense_layer = BatchNormalization()(dense_layer)\n",
    "    dense_layer = BatchNormalization()(dense_layer)\n",
    "    dense_layer = Activation('relu')(dense_layer)\n",
    "    dense_layer = Dropout(hparams[\"HP_DROPOUT\"])(dense_layer)\n",
    "    output = Dense(1, activation='sigmoid')(dense_layer)\n",
    "    \n",
    "    model = Model(inputs=[word_embedding_input], outputs=[output])\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(X_train, num_of_unique_chars = num_of_char, embedding_dimension = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=hparams[\"HP_OPTIMIZER\"], loss='mean_squared_error', metrics=[RootMeanSquaredError(name='rmse')])\n",
    "        \n",
    "log_dir_path = \"lstm_model/logs\"\n",
    "model_path = \"lstm_model\"\n",
    "model_weight_path = \"lstm_model/model_weights\"\n",
    "        \n",
    "tensor_board = TensorBoard(histogram_freq=1, write_graph=True, write_images=False)\n",
    "early_stopping = EarlyStopping(patience=3, restore_best_weights=True)\n",
    "reduce_lr_on_plateau = ReduceLROnPlateau(patience=3)\n",
    "        \n",
    "if verbose:\n",
    "    model.fit([X_train], [y_train], epochs=100, batch_size = 1024,\\\n",
    "verbose=1, shuffle=True, validation_data=([X_val], [y_val]), callbacks=[early_stopping, reduce_lr_on_plateau])\n",
    "        \n",
    "else: \n",
    "    model_history[forecast_start_date][direction] = model.fit([X_train], [y_train], epochs=100, batch_size = 1024,\\\n",
    "verbose=2, shuffle=True, validation_data=([X_val], [y_val]), callbacks=[early_stopping, reduce_lr_on_plateau])\n",
    "    \n",
    "\n",
    "# Saving model parameters..\n",
    "if not os.path.exists(os.path.join(os.getcwd(),model_path)):\n",
    "    os.makedirs(os.path.join(os.getcwd(),model_path))\n",
    "model_json = model.to_json()\n",
    "\n",
    "with open(os.path.join(os.getcwd(),model_path,\"model.json\"), \"w\") as json_file:\n",
    "          json_file.write(model_json)\n",
    "    \n",
    "if not os.path.exists(os.path.join(os.getcwd(),model_weight_path)):\n",
    "    os.makedirs(os.path.join(os.getcwd(),model_weight_path))\n",
    "    \n",
    "# serialize weights to HDF5\n",
    "model.save_weights(os.path.join(os.path.join(os.getcwd(),model_weight_path, \"model_weights.h5\")))      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame({\n",
    "                        \"Predictions\": np.vectorize(index_labels.get)(np.round(preds.reshape(-1,))), \n",
    "                        \"GrandTruth\": np.vectorize(index_labels.get)(np.array(y_test))\n",
    "                       })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Junk Word Classifier"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Markov Chain Probabilistic Model\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turkish_char_lookup_list = \"a,b,c,ç,d,e,f,g,ğ,h,ı,i,j,k,l,m,n,o,ö,p,r,s,ş,t,u,ü,v,y,z, \".split(\",\")\n",
    "english_char_lookup_list = \"a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w,x,y,z\".split(\",\")\n",
    "char_list = list(set.union(set(turkish_char_lookup_list)).union(set(english_char_lookup_list)))\n",
    "\n",
    "char_indices = dict([(char, idx) for idx, char in enumerate(char_list)])\n",
    "\n",
    "print(\"char_indices = {}\".format(char_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_of_turkish_and_english_words = max([max([len(str(kelime)) for kelime in turkish_words[\"Vocab\"]])] +\n",
    "                                              [max([len(str(word)) for word in english_words[\"Vocab\"]])])\n",
    "\n",
    "print(\"max_length_of_turkish_and_english_words = {} \".format(max_length_of_turkish_and_english_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_words = pd.read_csv(\"Armut_ML_Case_-_Eng.csv\")\n",
    "english_words.rename(columns = {\"Words\":\"Vocab\"}, inplace = True)\n",
    "\n",
    "english_words.head()\n",
    "\n",
    "turkish_words = pd.read_csv(\"Armut_ML_Case_-_Turkish.csv\")\n",
    "turkish_words.rename(columns = {\"Kelimeler\":\"Vocab\"}, inplace = True)\n",
    "\n",
    "turkish_words.head()\n",
    "\n",
    "raw_vocab_df = turkish_words.append(english_words).reset_index(drop=True)\n",
    "raw_vocab_df[\"Vocab\"] = raw_vocab_df[\"Vocab\"].astype(str)\n",
    "\n",
    "raw_vocab_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(raw_vocab_df, test_size=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"length of X_train = {}\".format(len(X_train)), \"length of X_test = {}\".format(len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sentence_into_chars(line):\n",
    "\n",
    "    return [char.lower() for char in line if char.lower() in char_list] \n",
    "\n",
    "def ngram(n_gram, word):\n",
    "    \n",
    "    n_gram_list = []\n",
    "    word_char_list = convert_sentence_into_chars(word)\n",
    "\n",
    "    for start in range(0, len(word_char_list) - n_gram + 1):\n",
    "        n_gram_list.append((word_char_list[start], word_char_list[start + 1]))\n",
    "        \n",
    "    return n_gram_list\n",
    "        \n",
    "def avg_transition_prob(l, log_prob_matrix):\n",
    "\n",
    "    log_prob = 0.0\n",
    "    number_of_transition = 0\n",
    "    for first_char, second_char in ngram(2, l):\n",
    "        log_prob += log_prob_matrix[char_indices[first_char]][char_indices[second_char]]\n",
    "        number_of_transition += 1\n",
    "\n",
    "    return math.exp(log_prob / (number_of_transition or 1))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def markov_chain_model(word_list):\n",
    "    \n",
    "    # we assume that each n_gram split has occur at least once.\n",
    "    log_prob_matrix = [[1 for i in range(len(char_list))] for i in range(len(char_list))]\n",
    "\n",
    "    for word in word_list:\n",
    "        n_gram_list=ngram(2, word)\n",
    "        for first_char, second_char in n_gram_list:\n",
    "            log_prob_matrix[char_indices[first_char]][char_indices[second_char]] += 1\n",
    "        \n",
    "    # Calculating log probabiities\n",
    "    for index, row in enumerate(log_prob_matrix):\n",
    "        total_occurences = float(sum(row))\n",
    "        for char in range(len(row)):\n",
    "            row[char] = math.log(row[char] / total_occurences)\n",
    "            \n",
    "    # They are selected from Armut_ML_Case_-_Eng.csv and Armut_ML_Case_-_Turkish.csv\n",
    "    genuine_word_samples = ['aslihan','two models','buraya güzel bir şey yazmak istiyorum','I want to say something','a b c']\n",
    "    junk_word_samples = ['asdfgh','cvbnmö','zxcvnadtruqe','ertyuıopğü','qwer <>zxcvb']\n",
    "\n",
    "    # Find the probability of generating a few arbitrarily choosen good and bad phrases.\n",
    "    good_probs = [avg_transition_prob(line, log_prob_matrix) for line in genuine_word_samples]\n",
    "    bad_probs = [avg_transition_prob(line, log_prob_matrix) for line in junk_word_samples]\n",
    "\n",
    "    # And pick a threshold halfway between the worst good and best bad inputs.\n",
    "    threshold = (min(good_probs) + max(bad_probs)) / 2\n",
    "    pickle.dump({'log_prob_matrix': log_prob_matrix, 'threshold': threshold}, open('junk_and_genuine_word_classifier_model.pkl', 'wb'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_chain_model(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "junk_and_genuine_word_classifier_model = pickle.load(open('junk_and_genuine_word_classifier_model.pkl', 'rb'))\n",
    "\n",
    "log_prob_matrix = junk_and_genuine_word_classifier_model['log_prob_matrix']\n",
    "threshold = junk_and_genuine_word_classifier_model['threshold']\n",
    "\n",
    "preds = []\n",
    "\n",
    "for test_word in X_test[\"Vocab\"]:\n",
    "    transition_prob = avg_transition_prob(test_word, log_prob_matrix)\n",
    "    if transition_prob > threshold:\n",
    "        preds.append(0)\n",
    "    else:\n",
    "        preds.append(1)    \n",
    "        \n",
    "preds_df = pd.DataFrame({\"Predictions\":preds, \"GrandTruth\": X_test[\"Vocab\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation (Brownian Motion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Bt2 − Bt1 = N(0,t2 − t1 ), where N(0,t2 − t1) is a normal distribution with variance t2 − t1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def brownianMotion(timePoints):\n",
    "    \n",
    "    x_start_point = 1\n",
    "    y_start_poin = 1\n",
    "    \n",
    "    brownianTrajectory = [(x_start_point, y_start_poin)]\n",
    "    \n",
    "    for t in range(len(timePoints)-1):\n",
    "        random_number_x = random.gauss(0, timePoints[t+1]-timePoints[t])\n",
    "        random_number_y= random.gauss(0, timePoints[t+1]-timePoints[t])        \n",
    "        brownianTrajectory.append((brownianTrajectory[t][0] + random_number_x, brownianTrajectory[t][1] + random_number_y))\n",
    "\n",
    "    return brownianTrajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timePoints = [time for time in np.arange(1,100,0.01)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brownianTrajectory = brownianMotion(timePoints)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
